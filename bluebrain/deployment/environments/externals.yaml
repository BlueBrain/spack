spack:
  view: false
  modules:
    default:
      tcl:
        include:
          - arm-forge
          - apptainer
          - bison
          - blender
          - ccache
          - clang-tools
          - cli-tools
          - cmake
          - cuda
          - cudnn
          - darshan-runtime
          - darshan-util
          - doxygen
          - environment-modules
          - flex
          - ffmpeg
          - gdb
          - git
          - git-lfs
          - gmake
          - graphviz
          - hpe-mpi
          - intel-oneapi-mkl
          - intel-oneapi-tbb
          - ispc
          - julia
          - likwid
          - llvm
          - mvapich2
          - ninja
          - node-js
          - nvhpc
          - openjdk
          - pgi
          - python
          - singularityce
          - snakemake
          - stat
          - totalview
          - valgrind
          - virtualgl
          - ilmbase
          - openblas
          - openexr
        cuda:
          environment:
            append_path:
              LD_LIBRARY_PATH: '{prefix}/extras/CUPTI/lib64'
            set:
              NVHPC_CUDA_HOME: '{prefix}'
        hpe-mpi:
          environment:
            prepend_path:
              LD_LIBRARY_PATH: '{prefix}/lib'
  packages:
    all:
      providers:
        jpeg: [libjpeg-turbo]
        lapack: [intel-mkl]
    cairo:
      variants: +pdf+png
    graphviz:
      variants: +pangocairo+expat
    llvm:
      # The general cuda should be set via the generic packages.yaml
      variants: +cuda cuda_arch=70 +omp_tsan +python +link_dylib
    mvapich2:
      variants: fabrics=mrail process_managers=slurm file_systems=gpfs
    pgi:
      variants: +network+nvidia+mpi
  specs:
    - llvm@14.0.6 ~lldb  # thanks, py-libclang (+lldb fails to link ncurses)
    - llvm@15.0.4
    # keep the x.y.z versions in sync with the llvm versions that we install
    # and the pN version in sync with the clang-tools recipe
    - clang-tools@15.0.4
    - nvhpc@23.1
    - arm-forge+accept-eula
    - apptainer
    - bison
    - blender
    - boost~mpi
    - bzip2
    - ccache
    - cgal
    - cli-tools
    - cmake
    # NVHPC 22.11 and 23.1 ship CUDA 11.8.0 (version.json)
    # CUDA 11.x do not support GCC 12
    - cuda@11.8.0%gcc@11.3.0
    # NVHPC 23.1 ships CUDA 12.0.0 (version.json)
    - cuda@12.0.0
    - cudnn@8.0.3.33-11.0
    - darshan-runtime
    - darshan-util
    - doxygen
    - environment-modules@4.5.1
    - ffmpeg+libx264
    - flex
    - freetype
    - gdb~python
    - git
    - git-lfs
    - glew
    - gmake  # Used to parallelize Spack deployment builds
    - graphviz
    - hdf5~mpi@1.10.7
    - help2man
    - hpe-mpi@2.25.hmpt
    - intel-mkl
    # Leads to issues down the line, where it is preferred over hpe-mpi
    # #providers
    # - intel-mpi
    - intel-oneapi-mkl@2021.4.0
    - intel-oneapi-tbb@2021.4.0
    # Ospray's preference
    - ispc@1.18.0^llvm@14
    # If julia's curl does not match what they normally ship, downloading behind a proxy
    # will break.
    - julia@1.6^curl@7.73  # As requested by Polina S.
    - libxslt
    - likwid
    - mvapich2@2.3.7
    - ncurses
    - ninja
    - node-js
    - openscenegraph  # doesn't like to be built in a spec with LLVM
    # Leads to issues down the line, where it is preferred over intel-mkl
    # #providers
    - openblas
    - openjdk@17  # HELP-17046
    - poppler
    - python
    - qhull
    - readline
    - singularityce
    - snakemake
    - sqlite
   # BlueBrain TODO: re-enable and fix
   #- stat
    - totalview
    - valgrind~mpi
    - virtualgl
    - zlib
    # Viz stuff that complicates specs
    - optix
    - ilmbase
    - openexr
