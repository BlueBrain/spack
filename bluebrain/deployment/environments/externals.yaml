spack:
  view: false
  modules:
    default:
      tcl:
        include:
          # - apptainer
          - bison
          - blender
          - ccache
          - clang-tools
          - cli-tools
          - cmake
          - cuda
          - darshan-runtime
          - darshan-util
          - doxygen
          - environment-modules
          - flex
          - ffmpeg
          - gdb
          - git
          - git-lfs
          - graphviz
          - hpe-mpi
          - intel-oneapi-mkl
          - intel-oneapi-tbb
          - ispc
          - julia
          - likwid
          - linaro-forge
          - llvm
          - mvapich2
          - ninja
          - node-js
          - nvhpc
          - openjdk
          - python
          - singularityce
          - stat
          - totalview
          - valgrind
          - virtualgl
          - ilmbase
          - openblas
          - openexr
        cuda:
          environment:
            append_path:
              LD_LIBRARY_PATH: '{prefix}/extras/CUPTI/lib64'
            set:
              NVHPC_CUDA_HOME: '{prefix}'
        hpe-mpi:
          environment:
            prepend_path:
              LD_LIBRARY_PATH: '{prefix}/lib'
        linaro-forge:
          environment:
            set:
              ALLINEA_USE_SSH_STARTUP: "1"
            unset:
              - HTTP_PROXY
              - HTTPS_PROXY
              - http_proxy
              - https_proxy
  packages:
    all:
      providers:
        jpeg: [libjpeg-turbo]
    cairo:
      variants: +pdf+png
    graphviz:
      variants: +pangocairo+expat
    llvm:
      # The general cuda should be set via the generic packages.yaml
      variants: +cuda cuda_arch=70 +omp_tsan +link_dylib
    mvapich2:
      variants: fabrics=mrail process_managers=slurm file_systems=gpfs
  specs:
    - llvm@17.0.4
    # keep the x.y.z versions in sync with the llvm versions that we install
    # and the pN version in sync with the clang-tools recipe
    - clang-tools@17.0.4
    # last known version of NVHPC to work with modern Neuron code
    - nvhpc@23.1
    # - apptainer
    - bison
    - blender
    - boost~mpi
    # 2024 deployment: 4.8 fails with a compiler error
    - ccache@4.7
    - cli-tools
    - cmake
    # NVHPC 23.9 ships CUDA 12.2.0 (version.json)
    # NVHPC 23.1 ships CUDA 12.0.0 (version.json)
    - cuda@12.0.0
    - darshan-runtime
    - darshan-util
    - doxygen
    - environment-modules@4.5.1
    - ffmpeg+libx264
    - flex
    - freetype
    - gdb~python
    - git
    - git-lfs
    - graphviz
    - hdf5~mpi
    - hpe-mpi@2.27.p1.hmpt
    - intel-oneapi-mkl@2023.2.0
    - intel-oneapi-tbb@2021.10.0
    # For Ospray`
    - ispc
    - julia@1.9^[virtuals=blas,lapack] openblas
    - likwid
    - linaro-forge+accept-eula
    - mvapich2
    - ninja
    - node-js
    # Leads to issues down the line, where it is preferred over intel-mkl
    # #providers
    - openblas
    - openjdk@17  # HELP-17046
    - poppler
    - python@3.8
    - python@3.9
    - python@3.10
    - python@3.11
    - python@3.12
    - singularityce
    # BlueBrain TODO: re-enable and fix
    # - stat
    # - totalview
    - valgrind~mpi
    - ilmbase
    - openexr
